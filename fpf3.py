# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UsM0ilw0mstipJYYsIk3q6WLUpx-gWJF
"""

import streamlit as st
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import load_model
import joblib
import matplotlib.pyplot as plt
import os

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# --- Helper Functions from the Notebook ---

@st.cache_data
def load_and_preprocess_data(ticker):
    """
    Loads, cleans, and preprocesses stock data from Yahoo Finance.
    This function mirrors the logic in the provided notebook.
    """
    try:
        # Download data with explicit parameters
        data = yf.download(ticker, period='5y', progress=False)

        if data is None or data.empty:
            st.error(f"No data retrieved for {ticker}")
            return None, None, None

        # Handle MultiIndex columns if present
        if isinstance(data.columns, pd.MultiIndex):
            data.columns = data.columns.droplevel(1)

        # Clean and preprocess
        df = data.copy()
        df = df[~df.index.duplicated(keep='last')]
        df = df.sort_index()

        # Check if required columns exist
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            st.error(f"Missing columns in data: {missing_columns}")
            return None, None, None

        core_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        df[core_columns] = df[core_columns].ffill()
        df = df.dropna(subset=core_columns)
        price_cols = ['Open', 'High', 'Low', 'Close']
        df = df[(df[price_cols] > 0).all(axis=1)]
        df['price_change'] = df['Close'].pct_change()
        df['price_change'] = df['price_change'].clip(-0.5, 0.5)

        if 'Volume' in df.columns:
            median_volume = df['Volume'][df['Volume'] > 0].median()
            df.loc[df['Volume'] <= 0, 'Volume'] = median_volume

        # Feature engineering
        df['returns'] = df['Close'].pct_change()
        df['volatility'] = df['returns'].rolling(window=5).std() * np.sqrt(5)
        df['sma_10'] = df['Close'].rolling(window=10).mean()
        df['sma_20'] = df['Close'].rolling(window=20).mean()
        df['price_ratio'] = df['Close'] / df['sma_20']
        df['volume_sma'] = df['Volume'].rolling(window=10).mean()
        df['volume_ratio'] = df['Volume'] / df['volume_sma']
        df['momentum_5'] = df['Close'] / df['Close'].shift(5)
        df['momentum_10'] = df['Close'] / df['Close'].shift(10)
        df['hl_ratio'] = (df['High'] - df['Low']) / df['Close']
        df['ticker'] = ticker
        df.columns.name = None
        df = df.dropna()

        # Add lag features
        for lag in [1, 2, 3, 5]:
            df[f'volatility_lag_{lag}'] = df['volatility'].shift(lag)
        df['volatility_ma_5'] = df['volatility'].rolling(window=5).mean()
        df['volatility_ma_10'] = df['volatility'].rolling(window=10).mean()
        df['volatility_std_5'] = df['volatility'].rolling(window=5).std()
        for lag in [1, 2, 3]:
            df[f'returns_lag_{lag}'] = df['returns'].shift(lag)

        df = df.dropna()

        # Define features and target
        feature_columns = [
            'returns', 'price_ratio', 'volume_ratio', 'momentum_5', 'momentum_10',
            'hl_ratio', 'volatility_lag_1', 'volatility_lag_2', 'volatility_lag_3',
            'volatility_lag_5', 'volatility_ma_5', 'volatility_ma_10', 'volatility_std_5',
            'returns_lag_1', 'returns_lag_2', 'returns_lag_3'
        ]
        target_column = 'volatility'

        return df, feature_columns, target_column

    except Exception as e:
        st.error(f"Error loading data for {ticker}: {str(e)}")
        return None, None, None

def create_demo_model_and_scalers(stock_option, feature_columns):
    """
    Creates demo model and scalers when the actual files don't exist.
    """
    try:
        # Create a simple demo LSTM model
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(50, input_shape=(1, len(feature_columns))),
            tf.keras.layers.Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')

        # Create demo scalers
        feature_scaler = MinMaxScaler()
        target_scaler = MinMaxScaler()

        # Fit with dummy data
        dummy_features = np.random.randn(100, len(feature_columns))
        dummy_target = np.random.randn(100, 1)

        feature_scaler.fit(dummy_features)
        target_scaler.fit(dummy_target)

        return model, feature_scaler, target_scaler

    except Exception as e:
        st.error(f"Error creating demo model: {str(e)}")
        return None, None, None

# --- Main Streamlit App Function ---
def main():
    st.set_page_config(layout="wide")
    st.title("Tanabyt Forecasting")
    st.markdown("### Predicting using an LSTM Model")

    # --- Sidebar for User Input ---
    st.sidebar.header("User Input")
    stock_option = st.sidebar.selectbox(
        "Select a stock:",
        ("QQQ", "SPY")
    )

    forecast_period = st.sidebar.slider(
        "Select volatility forecast period (days):",
        min_value=5, max_value=10, value=5, step=5
    )

    # Add option to use historical data for better performance demonstration
    use_historical = st.sidebar.checkbox(
        "Use historical data (for better performance demo)",
        value=False,
        help="Use older data where the model performs better, rather than most recent data"
    )

    if use_historical:
        historical_offset = st.sidebar.slider(
            "Historical data offset (days ago):",
            min_value=30, max_value=365, value=180, step=30,
            help="How many days back to use for prediction"
        )
    else:
        historical_offset = 0

    # --- Load Models and Scalers and Run App Logic ---
    try:
        # Updated file paths to match your actual file names
        if stock_option == "QQQ":
            model_path = "lstm_model_QQQ.keras"
            feature_scaler_path = "lstm_feature_scaler_QQQ.joblib"
            target_scaler_path = "lstm_target_scaler_QQQ.joblib"
        else:  # SPY
            model_path = "lstm_model_SPY.keras"
            feature_scaler_path = "lstm_feature_scaler_SPY.joblib"
            target_scaler_path = "lstm_target_scaler_SPY.joblib"

        # Check if model files exist
        model_exists = os.path.exists(model_path)
        feature_scaler_exists = os.path.exists(feature_scaler_path)
        target_scaler_exists = os.path.exists(target_scaler_path)

        if not (model_exists and feature_scaler_exists and target_scaler_exists):
            st.warning(f"Model files not found for {stock_option}. Looking for:")
            st.write(f"- {model_path}")
            st.write(f"- {feature_scaler_path}")
            st.write(f"- {target_scaler_path}")
            st.info("Using demo model for demonstration purposes.")

            # Load data first to get feature columns for demo model
            data, feature_columns, target_column = load_and_preprocess_data(stock_option)
            if data is None:
                st.error("Unable to load stock data.")
                return

            model, feature_scaler, target_scaler = create_demo_model_and_scalers(stock_option, feature_columns)
            if model is None:
                st.error("Unable to create demo model.")
                return
        else:
            # Load the trained model and scalers
            st.success(f"✅ Loading trained model files for {stock_option}...")
            model = load_model(model_path)
            feature_scaler = joblib.load(feature_scaler_path)
            target_scaler = joblib.load(target_scaler_path)

            # Display model info
            st.info(f"📊 Model loaded: {model.name if hasattr(model, 'name') else 'LSTM Model'}")
            st.info(f"🔧 Feature scaler range: {feature_scaler.feature_range}")
            st.info(f"🎯 Target scaler range: {target_scaler.feature_range}")

            # Load data
            data, feature_columns, target_column = load_and_preprocess_data(stock_option)

        # --- Data Fetching and Prediction Logic ---
        if stock_option and model and data is not None:
            st.subheader(f"Analyzing {stock_option}")

            # Check if we have enough data
            sequence_length = 10
            min_required_data = sequence_length + forecast_period

            if len(data) < min_required_data:
                st.error(f"Insufficient data. Need at least {min_required_data} data points, but only have {len(data)}.")
                return

            st.write("### Actual vs Predicted Volatility")

            # Display model diagnostics
            st.write("### 🔍 Model Diagnostics")
            st.write(f"**Using Trained Model:** ✅ Yes")
            st.write(f"**Model Architecture:** {len(model.layers)} layers")
            st.write(f"**Input Shape:** {model.input_shape}")
            st.write(f"**Feature Count:** {len(feature_columns)}")
            st.write(f"**Sequence Length:** {sequence_length}")
            st.write(f"**Data Points Available:** {len(data)}")
            if use_historical:
                st.write(f"**Using Historical Data:** {historical_offset} days ago")
                st.info("📈 Using historical data - expect better performance similar to training results!")
            else:
                st.write(f"**Using Recent Data:** Most recent {forecast_period} days")
                st.warning("⚠️ Recent market data may be harder to predict than training data")

            # Show feature scaling info
            with st.expander("Feature Scaling Details"):
                st.write(f"Feature scaler min: {feature_scaler.data_min_[:5]}")  # First 5 features
                st.write(f"Feature scaler max: {feature_scaler.data_max_[:5]}")  # First 5 features

            # Get the data for prediction based on user choice
            if use_historical:
                # Use historical data from X days ago
                end_idx = len(data) - historical_offset
                start_idx = max(0, end_idx - sequence_length - forecast_period)
                if start_idx >= end_idx:
                    st.error("Not enough historical data for the selected offset.")
                    return
                selected_data = data.iloc[start_idx:end_idx]
                data_period_label = f"{historical_offset} days ago"
            else:
                # Use most recent data
                selected_data = data.tail(sequence_length + forecast_period)
                data_period_label = "most recent data"

            # Get the latest data for prediction
            latest_data = selected_data[feature_columns + [target_column]]

            # Prepare for prediction
            X_pred_raw = latest_data[feature_columns].values

            # Check for any NaN or infinite values and handle them
            if np.any(np.isnan(X_pred_raw)) or np.any(np.isinf(X_pred_raw)):
                st.warning("Data contains NaN or infinite values. Cleaning data...")
                X_pred_raw = np.nan_to_num(X_pred_raw, nan=0.0, posinf=1.0, neginf=-1.0)

            X_pred_scaled = feature_scaler.transform(X_pred_raw)

            # Reshape for LSTM input
            X_pred_scaled_reshaped = X_pred_scaled.reshape(X_pred_raw.shape[0], 1, X_pred_raw.shape[1])

            # Make predictions
            predicted_scaled = model.predict(X_pred_scaled_reshaped, verbose=0)
            predicted_volatility = target_scaler.inverse_transform(predicted_scaled).flatten()

            # Get actual values for comparison
            if use_historical:
                actual_volatility = selected_data[target_column].tail(forecast_period).values
                prediction_dates = selected_data.index[-forecast_period:]
            else:
                actual_volatility = data[target_column].tail(forecast_period).values
                prediction_dates = data.index[-forecast_period:]

            # Create plot data with proper alignment
            if len(actual_volatility) == len(predicted_volatility[-forecast_period:]):
                plot_data = pd.DataFrame({
                    "Actual Volatility": actual_volatility,
                    "Predicted Volatility": predicted_volatility[-forecast_period:]
                }, index=prediction_dates)

                # Plotting with Streamlit's line_chart
                st.line_chart(plot_data)

                # Calculate and display current metrics
                mse = mean_squared_error(actual_volatility, predicted_volatility[-forecast_period:])
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(actual_volatility, predicted_volatility[-forecast_period:])
                r2 = r2_score(actual_volatility, predicted_volatility[-forecast_period:])

                st.write(f"### Current Prediction Metrics ({data_period_label})")
                current_metrics = pd.DataFrame({
                    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],
                    'Current Performance': [f"{mse:.6f}", f"{rmse:.6f}", f"{mae:.6f}", f"{r2:.6f}"]
                })
                st.table(current_metrics)

                # Performance comparison
                if stock_option == "QQQ":
                    historical_r2 = 0.764282
                    historical_rmse = 0.010437
                else:
                    historical_r2 = 0.770471
                    historical_rmse = 0.009363

                if r2 >= historical_r2 * 0.8:  # Within 80% of training performance
                    st.success(f"🎯 Great performance! R² = {r2:.4f} (training: {historical_r2:.4f})")
                elif r2 >= historical_r2 * 0.5:  # Within 50% of training performance
                    st.warning(f"📊 Moderate performance. R² = {r2:.4f} (training: {historical_r2:.4f})")
                else:
                    st.error(f"📉 Lower performance. R² = {r2:.4f} (training: {historical_r2:.4f})")
                    if not use_historical:
                        st.info("💡 Try enabling 'Use historical data' for better performance demonstration")

            else:
                st.warning("Mismatch in data lengths. Showing predicted volatility only.")
                pred_data = pd.DataFrame({
                    "Predicted Volatility": predicted_volatility[-forecast_period:]
                }, index=prediction_dates)
                st.line_chart(pred_data)

            # --- Display Historical Model Metrics ---
            st.write("### Historical Model Performance")

            # These are the metrics from the provided notebook for demonstration.
            if stock_option == "QQQ":
                metrics_data = {
                    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],
                    'QQQ Train': [0.000057, 0.007544, 0.005661, 0.752383],
                    'QQQ Test': [0.000109, 0.010437, 0.006647, 0.764282]
                }
            else:  # SPY
                metrics_data = {
                    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],
                    'SPY Train': [0.000036, 0.005980, 0.004293, 0.720412],
                    'SPY Test': [0.000088, 0.009363, 0.005570, 0.770471]
                }

            metrics_df = pd.DataFrame(metrics_data)
            st.table(metrics_df)

            # Display recent data
            st.write("### Recent Stock Data")
            st.dataframe(data[['Open', 'High', 'Low', 'Close', 'Volume', 'volatility']].tail(10))

            # Display prediction details
            st.write(f"### Prediction Details ({data_period_label})")
            prediction_details = pd.DataFrame({
                'Date': prediction_dates,
                'Actual Volatility': actual_volatility if len(actual_volatility) == forecast_period else ['N/A'] * forecast_period,
                'Predicted Volatility': predicted_volatility[-forecast_period:],
                'Absolute Error': np.abs(actual_volatility - predicted_volatility[-forecast_period:]) if len(actual_volatility) == forecast_period else ['N/A'] * forecast_period
            })
            st.dataframe(prediction_details)

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        st.write("Please check that all required files are present and the data can be loaded successfully.")

        # Debug information
        st.write("### Debug Information")
        st.write(f"Looking for files:")
        st.write(f"- Model: {model_path if 'model_path' in locals() else 'Not set'}")
        st.write(f"- Feature scaler: {feature_scaler_path if 'feature_scaler_path' in locals() else 'Not set'}")
        st.write(f"- Target scaler: {target_scaler_path if 'target_scaler_path' in locals() else 'Not set'}")

if __name__ == "__main__":
    main()